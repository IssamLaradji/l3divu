Paper ID	Created	Last Modified	Paper Title	Abstract	Primary Contact Author Name	Primary Contact Author Email	Authors	Author Names	Author Emails	Primary Subject Area	Secondary Subject Areas	Conflicts	Assigned	Completed	% Completed	Bids	Discussion	Status	Requested For Author Feedback	Author Feedback Submitted?	Requested For Camera Ready	Camera Ready Submitted?	Requested For Presentation	Files	Number of Files	Supplementary Files	Number of Supplementary Files	Reviewers	Reviewer Emails	MetaReviewers	MetaReviewer Emails	SeniorMetaReviewers	SeniorMetaReviewerEmails	Q1 (Do you want the manuscript to be published at CVPR proceedings upon acceptance?)	Q2 (CVPR Accepted work)
2	1/26/2023 3:18:15 PM -08:00	4/6/2023 6:34:24 AM -07:00	Improving Cross-Domain Detection with Self-Supervised Learning	Cross-Domain Detection (XDD) aims to train a domain-adaptive object detector using unlabeled images from a target domain and labeled images from a source domain. Existing approaches achieve this either by aligning the feature maps or the region proposals from the two domains, or by transferring the style of source images to that of target images. In this paper, rather than proposing another method following the existing lines, we introduce a new framework complementary to existing methods. Our framework unifies some popular Self-Supervised Learning (SSL) techniques (e.g., rotation angle prediction, strong/weak data augmentation, mean teacher modeling) and adapts them to the XDD task. Our basic idea is to leverage the unsupervised nature of these SSL techniques and apply them simultaneously across domains (source and target) and models (student and teacher). These SSL techniques can thus serve as shared bridges that facilitate knowledge transfer between domains. More importantly, as these techniques are independently applied in each domain, they are complementary to existing domain alignment techniques that relies on interactions between domains (e.g., adversarial alignment). We perform extensive analyses on these SSL techniques and show that they significantly improve the performance of existing methods. In addition, we reach comparable or even better performance than the state-of-the-art methods when integrating our framework with an old well-established method.	Kai Li	li.gml.kai@gmail.com	Kai Li (NEC LABORATORIES AMERICA, INC)*; Curtis Wigington (Adobe Research); Chris Tensmeyer (Adobe Research); Handong Zhao (Adobe Research); Nikolaos Barmpalios (Adobe Document Cloud); Vlad I Morariu (Adobe Research); Varun Manjunatha (Adobe Research); YUN FU (Northeastern University)	Li, Kai*; Wigington, Curtis; Tensmeyer, Chris; Zhao, Handong; Barmpalios, Nikolaos; Morariu, Vlad I; Manjunatha, Varun; FU, YUN	li.gml.kai@gmail.com*; wigingto@adobe.com; tensmeye@adobe.com; hazhao@adobe.com; barmpali@adobe.com; morariu@adobe.com; vmanjuna@adobe.com; yunfu@ece.neu.edu			0	4	3	75	0	Disabled (0)	Accept (Archive)	No	No	Yes	Yes	No	PaperForReview.pdf (1,775,273 bytes)	1	supple.zip (5,258,197 bytes)	1	Basura Fernando (Agency for Science, Technology and Research, A*STAR, Singapore); Jiacheng Wei (Nanyang Technological University); Keiji Yanai (The University of Electro-Communications, Tokyo); Stephen Gould (Australian National University, Australia)	basuraf@gmail.com; jiacheng002@e.ntu.edu.sg; yanai@cs.uec.ac.jp; stephen.gould@anu.edu.au					Yes	No
3	1/27/2023 2:29:27 AM -08:00	4/10/2023 1:26:53 PM -07:00	Self-Supervised Video Similarity Learning	We introduce S^2VS, a video similarity learning approach with self-supervision. Self-Supervised Learning (SSL) is typically used to train deep models on a proxy task so as to have strong transferability on target tasks after fine-tuning. Here, in contrast to prior work, SSL is used to perform video similarity learning and address multiple retrieval and detection tasks at once with no use of labeled data. This is achieved by learning via instance-discrimination with task-tailored augmentations and the widely used InfoNCE loss together with an additional loss operating jointly on self-similarity and hard-negative similarity. We benchmark our method on tasks where video relevance is defined with varying granularity, ranging from video copies to videos depicting the same incident or event. We learn a single universal model that achieves state-of-the-art performance on all tasks, surpassing previously proposed methods that use labeled data. The code and pretrained models are publicly available at: https://github.com/gkordo/s2vs	Giorgos Kordopatis-Zilos	georgekordopatis@iti.gr	Giorgos Kordopatis-Zilos (ITI-CERTH)*; Giorgos Tolias (Czech Technical University in Prague, Faculty of Electrical Engineering, Visual Recognition Group); Christos Tzelepis (Queen Mary University of London); Yiannis  Kompatsiaris (CERTH-ITI); Ioannis Patras (Queen Mary University of London); Symeon Papadopoulos (Information Technologies Institute / Centre for Research & Technology - Hellas, GR)	Kordopatis-Zilos, Giorgos*; Tolias, Giorgos; Tzelepis, Christos; Kompatsiaris, Yiannis ; Patras, Ioannis; Papadopoulos, Symeon	georgekordopatis@iti.gr*; toliageo@fel.cvut.cz; c.tzelepis@qmul.ac.uk; ikom@iti.gr; i.patras@qmul.ac.uk; papadop@iti.gr			0	5	3	60	0	Disabled (0)	Accept (Archive)	No	No	Yes	Yes	No	s2vs_paper.pdf (3,456,132 bytes)	1	s2vs_sup_mat.zip (3,539,731 bytes)	1	Basura Fernando (Agency for Science, Technology and Research, A*STAR, Singapore); He Zhao (Vision Lab - York University, Canada); Keiji Yanai (The University of Electro-Communications, Tokyo); Razvan Caramalau (Imperial College); Touqeer Ahmad (University of Colorado, Colorado Springs)	basuraf@gmail.com; zhufl@cse.yorku.ca; yanai@cs.uec.ac.jp; r.caramalau18@imperial.ac.uk; sh.touqeerahmad@gmail.com					Yes	No
4	1/27/2023 5:04:19 PM -08:00	4/5/2023 2:24:55 PM -07:00	MEnsA: Mix-up Ensemble Average for Unsupervised Multi Target Domain Adaptation on 3D Point Clouds	Unsupervised domain adaptation (UDA) addresses the problem of distribution shift between the unlabelled target domain and labelled source domain. While the single target domain adaptation (STDA) is well studied in the literature for both 2D and 3D vision tasks, multi-target domain adaptation (MTDA) is barely explored for 3D data despite its wide real-world applications such as autonomous driving systems for various geographical and climatic conditions. We establish an MTDA baseline for 3D point cloud data by proposing to mix the feature representations from all domains together to achieve better domain adaptation performance by an ensemble average, which we call Mixup Ensemble Average or MEnsA. With the mixed representation, we use a domain classifier to improve at distinguishing the feature representations of source domain from those of target domains in a shared latent space. In empirical validations on the challenging PointDA-10 dataset, we showcase a clear benefit of our simple method over previous unsupervised STDA and MTDA methods by large margins (up to 17.10% and 4.76% on averaged over all domain shifts).	Ashish Sinha	ashish_sinha@sfu.ca	Ashish Sinha (Simon Fraser University)*; Jonghyun Choi (Yonsei University)	Sinha, Ashish*; Choi, Jonghyun	ashish_sinha@sfu.ca*; jc@yonsei.ac.kr			0	4	2	50	0	Disabled (0)	Accept (Archive)	No	No	Yes	Yes	No	cvprw_l3divu.pdf (886,522 bytes)	1	cvprw_l3d_ivu_supp.pdf (588,845 bytes)	1	Asim Kadav (NEC Labs); Wataru Shimoda (The University of Electro-Communications, Tokyo); Xuhua Huang (Meta); Yalın Baştanlar (Izmir Institute of Technology)	asimkadav@gmail.com; shimoda-k@mm.inf.uec.ac.jp; xhuangat@connect.ust.hk; yalinbastanlar@iyte.edu.tr					Yes	No
5	2/1/2023 3:09:35 PM -08:00	4/12/2023 11:52:35 AM -07:00	HNSSL: Hard Negative-Based Self-Supervised Learning	Recently, learning from vast unlabeled data, especially self-supervised learning, has been emerging and attracting widespread attention. Self-supervised learning followed by supervised fine-tuning on a few labeled examples can significantly improve label efficiency and outperform standard supervised training using fully annotated data. In this work, we present a novel hard negative-based self-supervised deep learning paradigm, named HNSSL. Specifically, we design a student-teacher network to generate a multi-view of the data for self-supervised learning and integrate an online hard negative pair mining into the training. Then we derive a new triplet-type loss considering both positive sample pairs and online mined hard negative sample pairs. Extensive experiments demonstrate the effectiveness of the proposed method and its components on ILSVRC-2012 based on the same backbone network.  Specifically, for the linear evaluation task, the proposed HNSSL with a ResNet-50 encoder achieves the top-1 accuracy of 77.1%, which outperforms its previous counterparts by 2.8%. For the semi-supervised learning task, HNSSL with a ResNet-50 encoder obtains the top-1 accuracy of 73.4%, which outperforms the previous ResNet-50 encoder-based semi-supervised learning results by 4.6% using only 10% labels. For the task of transfer learning with linear evaluation, HNSSL with a ResNet-50 encoder achieves the best accuracy on six of seven widely used transfer learning datasets, which averagely outperforms previous ResNet-50 encoder-based transfer learning results by 2.5%.	Wentao Zhu	wentaozhu91@gmail.com	Wentao Zhu (Amazon)*; Jingya Liu (City College of New York); Yufang Huang (Cornell University)	Zhu, Wentao*; Liu, Jingya; Huang, Yufang	wentaozhu91@gmail.com*; jingya0208@gmail.com; yfhuang1992new@gmail.com			0	4	2	50	0	Disabled (0)	Accept (Archive)	No	No	Yes	Yes	No	LaTeXGuidelines_for_Author_Response__1_ (2).pdf (1,001,640 bytes)	1		0	Asim Kadav (NEC Labs); Wataru Shimoda (The University of Electro-Communications, Tokyo); Xilin Chen (Institute of Computing Technology, Chinese Academy of Sciences); Yalın Baştanlar (Izmir Institute of Technology)	asimkadav@gmail.com; shimoda-k@mm.inf.uec.ac.jp; xlchen@ict.ac.cn; yalinbastanlar@iyte.edu.tr					Yes	No
6	2/2/2023 4:05:54 AM -08:00	4/6/2023 3:54:21 PM -07:00	Self-supervised 3D Human Pose Estimation from a Single Image	We propose a new self-supervised method for predicting 3D human body pose from a single image. The prediction network is trained from a dataset of unlabelled images depicting people in typical poses and a set of unpaired 2D poses. By minimising the need for annotated data, the method has the potential for rapid application to pose estimation of other articulated structures (e.g. animals). The self-supervision comes from an earlier idea exploiting consistency between predicted pose under 3D rotation. Our method is a substantial advance on state-of-the-art self-supervised methods in training a mapping directly from images, without limb articulation constraints or any 3D empirical pose prior. We compare performance with state-of-the-art self-supervised methods using benchmark datasets that provide images and ground-truth 3D pose (Human3.6M, MPI-INF-3DHP). Despite the reduced requirement for annotated data, we show that the method outperforms on Human3.6M and matches performance on MPI-INF-3DHP. Qualitative results on a dataset of human hands show the potential for rapidly learning to predict 3D pose for articulated structures other than the human body.	Jose  A Sosa Martinez	scjasm@leeds.ac.uk	Jose  A Sosa Martinez (University of Leeds)*; David C Hogg (University of Leeds)	Sosa Martinez, Jose  A*; Hogg, David C	scjasm@leeds.ac.uk*; D.C.Hogg@leeds.ac.uk			0	4	2	50	0	Disabled (0)	Accept (Archive)	No	No	Yes	Yes	No	workshop_cvpr_ready.pdf (4,677,912 bytes)	1	supplemental_material_workshop.pdf (12,042,813 bytes)	1	Biagio Brattoli (Amazon Web Service); Biplab Banerjee (Indian Institute of Technology, Bombay); Jeremy Dawson (West Virginia University); Tristan Sylvain (Montreal Institute for Learning Algorithms)	biagio.brattoli@gmail.com; getbiplab@gmail.com; jeremy.dawson@mail.wvu.edu; tristan.sylvain@gmail.com					Yes	No
8	2/3/2023 2:04:00 PM -08:00	4/12/2023 1:00:53 AM -07:00	Learning text-to-video retrieval from image captioning	We describe a protocol to study text-to-video retrieval training with unlabeled videos, where we assume (i) no access to labels for any videos, i.e., no access to the set of ground-truth captions, but (ii) access to labeled images in the form of text. Using image expert models is a realistic scenario given that annotating images is cheaper therefore scalable, in contrast to expensive video labeling schemes. Recently, zero-shot image experts such as CLIP have established a new strong baseline for video understanding tasks. In this paper, we make use of this progress and instantiate the image experts from two types of models: a text-to-image retrieval model to provide an initial backbone, and image captioning models to provide supervision signal into unlabeled videos. We show that automatically labeling video frames with image captioning allows text-to-video retrieval training, which adapts the features to the target domain at no manual annotation cost, consequently outperforming the strong zero-shot CLIP baseline. During training, we sample captions from multiple video frames that match best the visual content, and perform a temporal pooling over frame representations by scoring frames according to their relevance to each caption. We conduct extensive ablations to provide insights and demonstrate the effectiveness of this simple framework by outperforming the CLIP zero-shot baseline on text-to-video retrieval on three standard datasets, namely MSR-VTT and MSVD. Code and models will be made publicly available.	Lucas Ventura	lucas.ventura@enpc.fr	Lucas Ventura (ENPC)*; Cordelia Schmid (Inria/Google); Gul Varol (Ecole des Ponts ParisTech)	Ventura, Lucas*; Schmid, Cordelia; Varol, Gul	lucas.ventura@enpc.fr*; cordelia.schmid@inria.fr; gul.varol@enpc.fr			0	4	2	50	0	Disabled (0)	Accept (No Archive)	No	No	Yes	Yes	No	2023_L3D_IVU.pdf (1,656,559 bytes)	1		0	Biagio Brattoli (Amazon Web Service); Biplab Banerjee (Indian Institute of Technology, Bombay); Jeremy Dawson (West Virginia University); Tristan Sylvain (Montreal Institute for Learning Algorithms)	biagio.brattoli@gmail.com; getbiplab@gmail.com; jeremy.dawson@mail.wvu.edu; tristan.sylvain@gmail.com					No	No
10	2/7/2023 9:52:39 AM -08:00	4/10/2023 8:23:45 PM -07:00	GAPS: Few-Shot Incremental Semantic Segmentation via Guided Copy-Paste Synthesis	Few-shot incremental segmentation is the task of updating a segmentation model, as novel classes are introduced online over time with a small number of training images. Although incremental segmentation methods exist in the literature, they tend to fall short in the few-shot regime and when given partially-annotated training images, where only the novel class is segmented. This paper proposes a data synthesizer, Guided copy-And-Paste Synthesis (GAPS), that improves the performance of few-shot incremental segmentation in a model-agnostic fashion. Despite the great success of copy-paste synthesis in conventional offline visual recognition, we demonstrate substantially degraded performance of its naive extension in our online scenario, due to newly encountered challenges. To this end, GAPS (i) addresses the partial-annotation problem by leveraging copy-paste to generate fully-labeled data for training, (ii) helps augment the few images of novel objects by introducing a guided sampling process, and (iii) mitigates catastrophic forgetting by employing a diverse memory-replay buffer. Compared to existing state-of-the-art methods, GAPS dramatically boosts the novel IoU of baseline methods on established few-shot incremental segmentation benchmarks by up to 80%. More notably, GAPS maintains good performance in even more impoverished annotation settings, where only single instances of novel objects are annotated.	Ri-Zhao Qiu	rizhaoq2@illinois.edu	Ri-Zhao Qiu (University of Illinois at Urbana-Champaign)*; Peiyi Chen (University of Illinois at Urbana-Champaign); Wangzhe Sun (Vanderbilt University); Yu-Xiong Wang (University of Illinois at Urbana-Champaign); Kris Hauser (University of Illinois)	Qiu, Ri-Zhao*; Chen, Peiyi; Sun, Wangzhe; Wang, Yu-Xiong; Hauser, Kris	rizhaoq2@illinois.edu*; peiyic2@illinois.edu; wangzhe.sun@vanderbilt.edu; yxw@illinois.edu; kkhauser@illinois.edu			0	4	4	100	0	Disabled (0)	Accept (No Archive)	No	No	Yes	Yes	No	CVPRW_2023_GAPS_FINAL.pdf (5,093,349 bytes)	1	CVPRW_2023_GAPS_SUPP_FINAL.pdf (5,797,303 bytes)	1	Doyup Lee (Kakao Brain); Hazel Doughty (University of Amsterdam); Jake Snell (University of Toronto, Vector Institute); Weihao Li (Australian National University )	damien.re@kakaobrain.com; hazel.doughty@uva.nl; jsnell@cs.toronto.edu; weihao.li1@anu.edu.au					No	No
11	2/8/2023 7:11:31 PM -08:00	4/6/2023 9:30:36 PM -07:00	SimDE: A Simple Domain Expansion Approach for Single-source Domain Generalization	Single domain generalization challenges model generalizability to unseen target domains when only one source domain is provided for training. To tackle this problem, domain expansion is adopted to learn domain-invariant information by exposing the model to more domain variations, which is still under-explored in previous work. In this paper, we propose a new and simplified objective for learning the desirable domain expansions by generating unconfident samples through the combination of entropy maximization and cross-entropy minimization. We further devise a novel framework that trains a pair of generators from different views by switching the guidance from the dual classifiers. In this way, the resulting method called Simple Domain Expansion (SimDE) can learn diverse domain expansions effectively and efficiently. Extensive experiments on prevalent single domain generalization benchmarks demonstrate the superiority of our method by offering improved results over the state-of-the-arts methods.	Ya Zhang	ya_zhang@sjtu.edu.cn	Qinwei Xu (Cooperative Medianet Innovation Center, Shang hai Jiao Tong University); Ruipeng Zhang (Cooperative Medianet Innovation Center, Shang hai Jiao Tong University); Yi-Yan Wu (Communications Research Centre, Ottawa, Ontario, Canada); Ya Zhang (Cooperative Medianet Innovation Center, Shang hai Jiao Tong University)*; Ning Liu (Shanghai Jiao Tong University); Yan-Feng Wang (Cooperative medianet innovation center of Shanghai Jiao Tong University)	Xu, Qinwei; Zhang, Ruipeng; Wu, Yi-Yan; Zhang, Ya*; Liu, Ning; Wang, Yan-Feng	qinweixu@sjtu.edu.cn; zhangrp@sjtu.edu.cn; yiyan.wu@ieee.org; ya_zhang@sjtu.edu.cn*; ningliu@sjtu.edu.cn; wangyanfeng@sjtu.edu.cn			0	4	3	75	0	Disabled (0)	Accept (Archive)	No	No	Yes	Yes	No	Submission 11.pdf (741,566 bytes)	1	Supplementary 11.pdf (485,667 bytes)	1	Céline Hudelot (CentraleSupélec); Jake Snell (University of Toronto, Vector Institute); Jiafan Zhuang (Shantou University); Xun Xu (I2R, A-STAR)	celine.hudelot@centralesupelec.fr; jsnell@cs.toronto.edu; jfzhuang@stu.edu.cn; alex.xun.xu@gmail.com					Yes	No
12	2/14/2023 1:19:22 AM -08:00	4/8/2023 3:55:04 AM -07:00	Impact of Pseudo Depth on Open World Object Segmentation with Minimal User Guidance	Pseudo depth maps are depth map predicitions which are used as ground truth during training. In this paper we leverage pseudo depth maps in order to segment objects of classes that have never been seen during training. This renders our object segmentation task an open world task. The pseudo depth maps are generated using pretrained networks, which have either been trained with the full intention to generalize to downstream tasks (LeRes and MiDaS), or which have been trained in an unsupervised fashion on video sequences (MonodepthV2). In order to tell our network which object to segment, we provide the network with a single click on the object's surface on the pseudo depth map of the image as input. We test our approach on two different scenarios: One without the RGB image and one where the RGB image is part of the input. Our results demonstrate a considerably better generalization performance from seen to unseen object types when depth is used. On the Semantic Boundaries Dataset we achieve an improvement from 61.57 to 69.79 IoU score on unseen classes, when only using half of the training classes during training and performing the segmentation on depth maps only.	Robin Schön	Robin.Schoen@uni-a.de	Robin Schön (University of Augsburg)*; Katja Ludwig (University of Augsburg); Rainer Lienhart ("Universitat Augsburg, Germany")	Schön, Robin*; Ludwig, Katja; Lienhart, Rainer	Robin.Schoen@uni-a.de*; katja.ludwig@uni-a.de; Rainer.Lienhart@informatik.uni-augsburg.de			0	5	4	80	0	Disabled (0)	Accept (Archive)	No	No	Yes	Yes	No	Impact_of_Pseudo_Depth_on_Segmenting_Unknown_Objects_with_User_Guidance__CVPR2023_-2.pdf (5,319,950 bytes)	1	Impact_of_Pseudo_Depth_on_Segmenting_Unknown_Objects_with_User_Guidance__CVPR2023___Supplementary_Material_.pdf (196,468 bytes)	1	Céline Hudelot (CentraleSupélec); Jiafan Zhuang (Shantou University); Pau Rodriguez (Apple); Shuai Zheng (Cruise LLC); Xun Xu (I2R, A-STAR)	celine.hudelot@centralesupelec.fr; jfzhuang@stu.edu.cn; pau.rodri1@gmail.com; kyle@kylezheng.org; alex.xun.xu@gmail.com					Yes	No
16	2/28/2023 2:06:41 AM -08:00	4/5/2023 11:51:40 PM -07:00	An Effective Crop-Paste Pipeline for Few-shot Object Detection	Few-shot object detection (FSOD) aims to expand an object detector for novel categories given only a few instances for training. However, detecting novel categories with only a few samples usually leads to the problem of misclassification. In FSOD, we notice the false positive (FP) of novel categories is prominent, in which the base categories are often recognized as novel ones. To address this issue, a novel data augmentation pipeline that Crops the Novel instances and Pastes them on the selected Base images, called CNPB, is proposed. There are two key questions to be answered: (1) How to select useful base images? and (2) How to combine novel and base data? We design a multi-step selection strategy to find useful base data. Specifically, we first discover the base images which contain the FP of novel categories and select a certain amount of samples from them for the base and novel categories balance. Then the bad cases, such as the base images that have unlabeled ground truth or easily confused base instances, are removed by using CLIP. Finally, the same category strategy is adopted, in which a novel instance with category n is pasted on the base image with the FP of n. During combination, a novel instance is cropped and randomly down-sized, and thus pasted at the assigned optimal location from the randomly generated candidates in a selected base image. Our method is simple yet effective and can be easy to plug into existing FSOD methods, demonstrating significant potential for use. Extensive experiments on PASCAL VOC and MS COCO validate the effectiveness of our method.	Shaobo Lin	linshaobo@sensetime.com	Shaobo Lin (Sensetime Group Limited)*; Kun Wang (SenseTime Group Limited); Xingyu ZENG (SenseTime Group Limited); Rui Zhao (SenseTime Group Limited)	Lin, Shaobo*; Wang, Kun; ZENG, Xingyu; Zhao, Rui	linshaobo@sensetime.com*; wangkun@sensetime.com; zengxingyu@sensetime.com; zhaorui@sensetime.com			0	4	3	75	0	Disabled (0)	Accept (Archive)	No	No	Yes	Yes	No	An Effective Crop-Paste Pipeline for Few-shot Object Detection.pdf (1,328,116 bytes)	1		0	Dong-Jin Kim (Hanyang University); Guanbin Li (Sun Yat-sen University); Issam Hadj Laradji (ServiceNow); Nikita Araslanov (TU Munich)	djnjusa@gmail.com; liguanbin@mail.sysu.edu.cn; issam.laradji@gmail.com; nikita.araslanov@tum.de					Yes	No
18	3/1/2023 12:35:33 PM -08:00	4/9/2023 4:11:25 PM -07:00	Improving Data-Efficient Fossil Segmentation via Model Editing	Most computer vision research focuses on datasets containing thousands of images of commonplace objects. However, many high-impact datasets, such as those in medicine and the geosciences, contain fine-grain objects that require domain-expert knowledge to recognize and are time-consuming to collect and annotate. As a result, these datasets contain few labeled images, and current machine vision models cannot train intensively on them. Originally introduced to correct large-language models, model-editing techniques in machine learning have been shown to improve model performance using only small amounts of data and additional training. Using a Mask R-CNN to segment ancient reef fossils in rock sample images, we present a two-part  paradigm to improve fossil segmentation with few labeled images: we first identify model weaknesses using image perturbations and then mitigate those weaknesses using model editing. Specifically, we apply domain-informed image perturbations to expose the Mask R-CNN’s inability to distinguish between different classes of fossils and its inconsistency in segmenting fossils with different textures. To address these shortcomings, we extend an existing model-editing method for correcting systematic mistakes in image classification to image segmentation with no additional labeled data needed and show its effectiveness in decreasing confusion between different kinds of fossils. We also highlight the best settings for model editing in our situation: making a single edit using all relevant pixels in one image (vs. using multiple images, multiple edits, or fewer pixels). Though we focus on fossil segmentation, our approach may be useful in other similar fine-grain segmentation problems where data is limited.	Indu Panigrahi	indup@princeton.edu	Indu Panigrahi (Princeton University)*; Ryan A Manzuk (Princeton University); Adam C Maloof (Princeton University); Ruth C Fong (Princeton University)	Panigrahi, Indu*; Manzuk, Ryan A; Maloof, Adam C; Fong, Ruth C	indup@princeton.edu*; rmanzuk@princeton.edu; maloof@princeton.edu; ruthfong@cs.princeton.edu			0	4	2	50	0	Disabled (0)	Accept (Archive)	No	No	Yes	Yes	No	L3D_18.pdf (9,459,153 bytes)	1	L3D_18_SuppMat.pdf (88,788 bytes)	1	Chih-Hui Ho (University of California San Diego); Fanyi Xiao (Meta); Jongheon Jeong (KAIST); Pengwan Yang (University of Amsterdam)	chh279@eng.ucsd.edu; fyxiao@ucdavis.edu; jongheonj@kaist.ac.kr; yangpengwan2016@gmail.com					Yes	No
19	3/2/2023 2:03:51 AM -08:00	4/7/2023 7:32:20 AM -07:00	What Affects Learned Equivariance in Deep Image Recognition Models?	Equivariance w.r.t. geometric transformations in neural networks improves data efficiency, parameter efficiency and robustness to out-of-domain perspective shifts. When equivariance is not designed into a neural network, the network can still learn equivariant functions from the data. We quantify this learned equivariance, by proposing an improved measure for equivariance. We find evidence for a correlation between learned translation equivariance and validation accuracy on ImageNet. We therefore investigate what can increase the learned equivariance in neural networks, and find that data augmentation, reduced model capacity and inductive bias in the form of convolutions induce higher learned equivariance in neural networks.	Robert-Jan Bruintjes	R.Bruintjes@tudelft.nl	Robert-Jan Bruintjes (Delft University of Technology)*; Tomasz Motyka (Delft University of Technology); Jan C van Gemert (Delft University of Technology)	Bruintjes, Robert-Jan*; Motyka, Tomasz; van Gemert, Jan C	R.Bruintjes@tudelft.nl*; motyka.tomasz.1995@gmail.com; j.c.vangemert@tudelft.nl			0	4	2	50	0	Disabled (0)	Accept (Archive)	No	No	Yes	Yes	No	_CVPRW_2023__Learned_Equivariance_in_Convolutional_Neural_Networks (7).pdf (3,665,185 bytes)	1		0	Alice OTHMANI (UPEC); Joao Monteiro (ServiceNow); Jongheon Jeong (KAIST); Sung Whan Yoon (Ulsan National Institute of Science and Technology (UNIST))	alice.othmani@u-pec.fr; joaomonteirof@gmail.com; jongheonj@kaist.ac.kr; shyoon8@unist.ac.kr					Yes	No
20	3/2/2023 2:27:50 AM -08:00	4/10/2023 1:00:46 PM -07:00	Zero-shot Unsupervised Transfer Instance Segmentation	Segmentation is a core computer vision competency, with applications spanning a broad range of scientifically and economically valuable domains. To date, however, the prohibitive cost of annotation has limited the deployment of flexible segmentation models. In this work, we propose Zero-shot Unsupervised Transfer Instance Segmentation (ZUTIS), a framework that aims to meet this challenge. The key strengths of ZUTIS are: (i) no requirement for instance-level or pixel-level annotations; (ii) an ability of zero-shot transfer, i.e., no assumption on access to a target data distribution; (iii) a unified framework for semantic and instance segmentations with solid performance on both tasks compared to state-or-the art unsupervised methods. While comparing to previous work, we show ZUTIS achieves a gain of 2.2 mask AP on COCO-20K and 14.5 mIoU on ImageNet-S with 919 categories for instance and semantic segmentations, respectively. Code will be made publicly available.	Gyungin Shin	gyungin@robots.ox.ac.uk	Gyungin Shin (University of Oxford)*; Samuel Albanie (University of Cambridge); Weidi Xie (Shanghai Jiao Tong University)	Shin, Gyungin*; Albanie, Samuel; Xie, Weidi	gyungin@robots.ox.ac.uk*; sma71@cam.ac.uk; weidi@robots.ox.ac.uk			0	5	2	40	0	Disabled (0)	Accept (Archive)	No	No	Yes	Yes	No	paper_id_20.pdf (7,947,272 bytes)	1	paper_id_20_supp_mat.pdf (8,181,479 bytes)	1	Alice OTHMANI (UPEC); Joao Monteiro (ServiceNow); Mennatullah Siam (York University); Peng YUN (Hong Kong University of Science and Technology); Sung Whan Yoon (Ulsan National Institute of Science and Technology (UNIST))	alice.othmani@u-pec.fr; joaomonteirof@gmail.com; mennatul@ualberta.ca; pyun@connect.ust.hk; shyoon8@unist.ac.kr					Yes	No
22	3/2/2023 6:59:20 AM -08:00	4/6/2023 12:23:33 AM -07:00	Zero-Shot Action Recognition with Transformer-based Video Semantic Embedding	While video action recognition has been an active area of research for several years, zero-shot action recognition has only recently started gaining traction. In this work, we propose a novel end-to-end trained transformer model which is capable of capturing long range spatiotemporal dependencies efficiently, contrary to existing approaches which use 3D-CNNs. Moreover, to address a common ambiguity in the existing works about classes that can be considered as previously unseen, we propose a new experimentation setup that satisfies the zero-shot learning premise for action recognition by avoiding overlap between the training and testing classes. The proposed approach significantly outperforms the state of the arts in zero-shot action recognition in terms of the the top-1 accuracy on UCF-101, HMDB-51 and ActivityNet datasets.	Keval Doshi	KEVALDOSHI@MAIL.USF.EDU	Keval Doshi (University of South Florida)*; Yasin Yilmaz (University of South Florida)	Doshi, Keval*; Yilmaz, Yasin	KEVALDOSHI@MAIL.USF.EDU*; yasiny@usf.edu			0	4	3	75	0	Disabled (0)	Accept (Archive)	No	No	Yes	Yes	No	ZeroShot__CVPRW (1).pdf (1,349,249 bytes)	1	ZeroShot__CVPRW (2).pdf (544,744 bytes)	1	Bing Zhao (Inspur ); EU WERN TEH (University of Guelph); Hongguang Zhang (Australian National University); Jing Xu (Harbin Institute of Technology, Shenzhen)	zhaobing0412@outlook.com; euwern1987@gmail.com; zhang.hongguang@outlook.com; xujing.may@gmail.com					Yes	No
24	3/2/2023 8:42:41 AM -08:00	4/7/2023 8:32:41 AM -07:00	Contrast, Stylize and Adapt: Unsupervised Contrastive Learning Framework for Domain Adaptive Semantic Segmentation	To overcome the domain gap between synthetic and real-world datasets, unsupervised domain adaptation methods have been proposed for semantic segmentation. Majority of the previous approaches have attempted to reduce the gap either at the pixel or feature level, disregarding the fact that the two components interact positively. To address this, we present CONtrastive FEaTure and pIxel alignment (CONFETI) for bridging the domain gap at both the pixel and feature levels using a unique contrastive formulation. We introduce well-estimated prototypes by including category-wise cross-domain information to link the two alignments: the pixel-level alignment is achieved using the jointly trained style transfer module with the prototypical semantic consistency, while the feature-level alignment is enforced to cross-domain features with the pixel-to-prototype contrast. Our extensive experiments demonstrate that our method outperforms existing state-of-the-art methods using DeepLabV2. Our code has been made publicly available.	Tianyu Li	hugo_li@sjtu.edu.cn	Tianyu Li (Shanghai Jiao Tong University)*; Subhankar Roy (Telecom Paris); Huayi Zhou (Shanghai Jiao Tong University); Hongtao Lu (Shanghai Jiao Tong University); Stéphane Lathuilière (Telecom-Paris)	Li, Tianyu*; Roy, Subhankar; Zhou, Huayi; Lu, Hongtao; Lathuilière, Stéphane	hugo_li@sjtu.edu.cn*; subhankar.roy@telecom-paris.fr; sjtu_zhy@sjtu.edu.cn; htlu@sjtu.edu.cn; stephane.lathuiliere@telecom-paris.fr			0	4	4	100	0	Disabled (0)	Accept (Archive)	No	No	Yes	Yes	No	0024.pdf (28,141,678 bytes)	1	0024_supp.pdf (206,915 bytes)	1	Chen Chen (University of Central Florida); Kin Wai LAU  (TCL AI Lab); Xingyu Chen (Xi'an Jiaotong University); Yude Wang (Institute of Computing Technology Chinese Academy of Sciences)	chen.chen@crcv.ucf.edu; laukinwaisteven@gmail.com; xingyuchen1990@gmail.com; yude.wang@vipl.ict.ac.cn					Yes	No
25	3/2/2023 9:41:14 AM -08:00	4/10/2023 9:14:55 AM -07:00	Rethinking matching-based few-shot action recognition	Few-shot action recognition benefits from incorporating temporal information. Prior work either encodes such information in the representation itself and learns classifiers at test time or obtains frame-level features and performs pairwise temporal matching. We first evaluate several recent matching-based approaches using features from spatio-temporal backbones, a comparison missing from the literature, and show that the gap in performance between simple baselines and more complicated methods is significantly reduced. Inspired by this, we propose Chamfer++, a non-temporal matching function that achieves state-of-the-art results in few-shot action recognition. 	Juliette LD Bertrand	bertrjul@fel.cvut.cz	Juliette LD Bertrand (CTU)*; Yannis Kalantidis (NAVER LABS Europe); Giorgos Tolias (Czech Technical University in Prague, Faculty of Electrical Engineering, Visual Recognition Group)	Bertrand, Juliette LD*; Kalantidis, Yannis; Tolias, Giorgos	bertrjul@fel.cvut.cz*; ykalant@image.ntua.gr; toliageo@fel.cvut.cz			3	4	3	75	0	Disabled (0)	Accept (No Archive)	No	No	Yes	Yes	No	Rethinking_matching_based_few_shot_action_recognition_L3DIVU.pdf (345,140 bytes)	1		0	Guangyao Chen (Peking University); Shuzhi Yu (Duke University); Tarun Kalluri (UCSD); Xingyu Chen (Xi'an Jiaotong University)	gy.chen@pku.edu.cn; shuzhiyu@cs.duke.edu; tarun.05.kalluri@gmail.com; xingyuchen1990@gmail.com					No	No
26	3/2/2023 5:26:36 PM -08:00	4/10/2023 5:36:35 AM -07:00	OWL (Observe, Watch, Listen): Audiovisual Temporal Context for Localizing Actions in Egocentric Videos	Egocentric videos capture sequences of human activities from a first-person perspective and can provide rich multi-modal signals. However, most current localization methods use third-person videos and only incorporate visual information. In this work, we take a deep look into the effectiveness of audiovisual context in detecting actions in egocentric videos and introduce a simple-yet-effective approach via Observing, Watching, and Listening (OWL). OWL leverages audiovisual information and context for egocentric Temporal Action Localization (TAL). We validate our approach in two large-scale datasets, EPIC-KITCHENS and HOMAGE. Extensive experiments demonstrate the relevance of the audiovisual temporal context. Namely, we boost the localization performance (mAP) over visual-only models by +2.23% and +3.35% in the above datasets.	Merey Ramazanova	merey.ramazanova@kaust.edu.sa	Merey Ramazanova (KAUST)*; Victor A Escorcia (Samsung AI Center); Fabian Caba (Adobe Research); Chen Zhao (KAUST); Bernard Ghanem (KAUST)	Ramazanova, Merey*; Escorcia, Victor A; Caba, Fabian; Zhao, Chen; Ghanem, Bernard	merey.ramazanova@kaust.edu.sa*; victor.escorcia@kaust.edu.sa; caba@adobe.com; chen.zhao@kaust.edu.sa; Bernard.Ghanem@kaust.edu.sa			0	4	3	75	0	Disabled (0)	Accept (Archive)	No	No	Yes	Yes	No	OWL_CVPRW2023.pdf (2,998,528 bytes)	1	Archive.zip (27,013,274 bytes)	1	Guangyao Chen (Peking University); Shuzhi Yu (Duke University); Tarun Kalluri (UCSD); Yujiao Shi (ANU)	gy.chen@pku.edu.cn; shuzhiyu@cs.duke.edu; tarun.05.kalluri@gmail.com; yujiao.shi@anu.edu.au					Yes	No
27	3/2/2023 6:43:43 PM -08:00	4/8/2023 7:20:45 AM -07:00	Mutual Exclusive Modulator for Long-Tailed Recognition	The long-tailed recognition (LTR) is the task of learning high-performance classifiers given extremely imbalanced training samples between categories. Most of the existing works address the problem by either enhancing the features of tail classes or re-balancing the classifiers to reduce the inductive bias. In this paper, we try to look into the root cause of the LTR task, i.e., training samples for each class are greatly imbalanced, and propose a straightforward solution. We split the categories into three groups, i.e., many, medium and few, according to the number of training images. The three groups of categories are separately predicted to reduce the difficulty for classification. This idea naturally arises a new problem of how to assign a given sample to the right class groups? We introduce a mutual exclusive modulator which can estimate the probability of an image belonging to each group. Particularly, the modulator consists of a light-weight module and learned with a mutual exclusive objective. Hence, the output probabilities of the modulator encode the data volume clues of the training dataset. They are further utilized as prior information to guide the prediction of the classifier. We conduct extensive experiments on multiple datasets, e.g., ImageNet-LT, PlaceLT and iNaturalist 2018 to evaluate the proposed approach. Our method achieves competitive performance compared to the state-of-the-art benchmarks. 	Haixu Long	hxlong@mail.ustc.edu.cn	Haixu Long (University of Science and Technology of China)*; Xiaolin Zhang (UTS); Yanbin Liu (The University of Western Australia); Zongtai Luo (Sensetime); Jianbo Liu (The Chinese University of Hong Kong)	Long, Haixu*; Zhang, Xiaolin; Liu, Yanbin; Luo, Zongtai; Liu, Jianbo	hxlong@mail.ustc.edu.cn*; solli.zhang@gmail.com; csyanbin@gmail.com; luozongtai@hotmail.com; liujianbo@link.cuhk.edu.hk			0	4	2	50	0	Disabled (0)	Accept (Archive)	No	No	Yes	Yes	No	Mutual_Exclusive_Modulator_for_Long_Tailed_Recognition.pdf (553,441 bytes)	1		0	Fabio Cermelli (Politecnico di Torino); Lihe Yang (Nanjing University); Utkarsh Mall (Cornell University); Yujiao Shi (ANU)	fabio.cermelli@polito.it; lihe.yang.cs@gmail.com; ukm4@cornell.edu; yujiao.shi@anu.edu.au					Yes	No
28	3/2/2023 9:21:04 PM -08:00	4/9/2023 9:48:15 AM -07:00	Neural Transformation Network to Generate Diverse Views for Contrastive Learning	Recent unsupervised representation learning methods rely heavily on various transformations to generate distinctive views of given samples. Transformations for these views are generally defined manually, requiring significant human effort to design detailed configurations and validate practical efficacy. Furthermore, the diversity of these views is quite limited in scope causing the network to be invariant to only a small set of data transformations. To address these problems, we introduce a neural transformation network that learns to generate diverse views. Our proposed framework consists of an encoder-decoder network architecture that encodes semantic information and then randomly stylizes it with style amplification. However, such generative processes tend to cause degradation compared to the original images, which can harm the quality of the learned representation. To remedy this issue and generate more diverse styles, we use a linear augmentation between the generated view and the original image. Finally, we apply geometric transformations to aid in contrastive learning of representations. We evaluate the learned representations on various downstream vision tasks. Results show highly competitive recognition performance compared to the state-of-the-art methods that use learned views or hand-crafted views for representation learning.	Changick Kim	changick@kaist.ac.kr	Taekyung Kim (KAIST); Debasmit Das (Qualcomm AI Research); Seokeon Choi (Qualcomm AI Research); Minki Jeong (KAIST); Seunghan Yang (Qualcomm AI Research); Sungrack Yun (Qualcomm AI Research); Changick Kim (KAIST)*	Kim, Taekyung; Das, Debasmit; Choi, Seokeon; Jeong, Minki; Yang, Seunghan; Yun, Sungrack; Kim, Changick*	tkkim93@kaist.ac.kr; debadas@qti.qualcomm.com; seokchoi@qti.qualcomm.com; rhm033@kaist.ac.kr; seunghan@qti.qualcomm.com; sungrack@qti.qualcomm.com; changick@kaist.ac.kr*			6	5	2	40	0	Disabled (0)	Accept (Archive)	No	No	Yes	Yes	No	NTN_main.pdf (11,986,809 bytes)	1	NTN_supp.pdf (202,248 bytes)	1	Fabio Cermelli (Politecnico di Torino); John Just (Iowa State University); Lihe Yang (Nanjing University); Pau Rodriguez (Apple); Utkarsh Mall (Cornell University)	fabio.cermelli@polito.it; justjo@iastate.edu; lihe.yang.cs@gmail.com; pau.rodri1@gmail.com; ukm4@cornell.edu					Yes	No
31	3/3/2023 2:02:41 PM -08:00	4/8/2023 8:26:42 AM -07:00	Posture-based Infant Action Recognition in the Wild with Very Limited Data	Automatic detection of infant actions from home videos could aid medical and behavioral specialists in the early detection of motor impairments in infancy. However, most computer vision approaches for action recognition are centered around adult subjects, following datasets and benchmarks in the field. In this work, we present a data-efficient pipeline for infant action recognition based on the idea of modeling an action as a time sequence consisting of two different stable postures with a transition period between them. The postures are detected frame-wise from the estimated 2D and 3D infant body poses and the action sequence is segmented based on the posture-driven low-dimensional features of each frame. To spur further research in the field, we also created and release the first-of-its-kind infant action dataset---InfAct---consisting of 200 fully annotated home videos representing a wide range of common infant actions, intended as a public benchmark. Among the ten more common classes of infant actions, our action recognition model achieved 78.0\% accuracy when tested on InfAct, highlighting the promise of video-based infant action recognition as a viable monitoring tool for infant motor development.   	Sarah Ostadabbas	ostadabbas@ece.neu.edu	Xiaofei Huang (Northeastern University); Lingfei Luan (Northeastern University ); Elaheh Hatamimajoumerd (Northeastern University); Michael Wan (The Roux Institute at Northeastern University); Pooria Daneshvar Kakhaki (Northeastern University); Rita Obeid (Case Western Reserve University); Sarah Ostadabbas (Northeastern University)*	Huang, Xiaofei; Luan, Lingfei; Hatamimajoumerd, Elaheh; Wan, Michael; Daneshvar Kakhaki, Pooria; Obeid, Rita; Ostadabbas, Sarah*	xhuang@ece.neu.edu; l.luan@northeastern.edu; e.hatamimajoumerd@northeastern.edu; mi.wan@northeastern.edu; daneshvarkakhaki.p@northeastern.edu; rxo76@case.edu; ostadabbas@ece.neu.edu*			0	3	3	100	0	Disabled (0)	Accept (Archive)	No	No	Yes	Yes	No	InfantAction_CVPRW_2023_6_Final.pdf (6,056,973 bytes)	1		0	Charig Yang (University of Oxford); Pinzhuo Tian (Shanghai University); Yuhe Jin (University of British Columbia)	charig@robots.ox.ac.uk; pinzhuo@shu.edu.cn; yuhejin@cs.ubc.ca					Yes	No
32	3/4/2023 9:16:08 AM -08:00	4/10/2023 11:58:42 PM -07:00	NIFF: Alleviating Forgetting in Generalized Few-Shot Object Detection via Neural Instance Feature Forging	Privacy and memory are two recurring themes in a broad conversation about the societal impact of AI. These concerns arise from the need for huge amounts of data to train deep neural networks. A promise of Generalized Few-shot Object Detection (G-FSOD), a learning paradigm in AI, is to alleviate the need for collecting abundant training samples of novel classes we wish to detect by leveraging prior knowledge from old classes (i.e., base classes). G-FSOD strives to learn these novel classes while alleviating catastrophic forgetting of the base classes. However, existing approaches assume that the base images are accessible, an assumption that does not hold when sharing and storing data is problematic. In this work, we propose the first data-free knowledge distillation (DFKD) approach for G-FSOD that leverages the statistics of the region of interest (RoI) features from the base model to forge instance-level features without accessing the base images. Our contribution is three-fold: (1) we design a standalone lightweight generator with (2) class-wise heads (3) to generate and replay diverse instance-level base features to the RoI head while finetuning on the novel data. This stands in contrast to standard DFKD approaches in image classification, which invert the entire network to generate base images. Moreover, we make careful design choices in the novel finetuning pipeline to regularize the model. We show that our approach can dramatically reduce the base memory requirements, all while setting a new standard for G-FSOD on the challenging MS-COCO and PASCAL-VOC benchmarks.  	Karim Guirguis	karim.guirguis@de.bosch.com	Karim Guirguis (Robert Bosch Coorporate Research)*; Johannes Meier (Robert Bosch Coorporate Research); George Eskandar (University of Stuttgart); Matthias Kayser (Robert Bosch Coorporate Research); Bin Yang (University of Stuttgart); Jürgen Beyerer (Fraunhofer IOSB)	Guirguis, Karim*; Meier, Johannes; Eskandar, George; Kayser, Matthias; Yang, Bin; Beyerer, Jürgen	karim.guirguis@de.bosch.com*; meier.johannes@yahoo.de; george.eskandar@iss.uni-stuttgart.de; Matthias.Ochs2@de.bosch.com; bin.yang@iss.uni-stuttgart.de; juergen.beyerer@iosb.fraunhofer.de			1	0	0	0	0	Disabled (0)	Accept (No Archive)	No	No	Yes	Yes	No	CVPRW_2023_NIFF__Alleviating_Forgetting_in_Generalized_Few_Shot_Object_Detection_via_Neural_Instance_Feature_Forging.pdf (824,417 bytes)	1	CVPRW_2023_Supp_NIFF__Alleviating_Forgetting_in_Generalized_Few_Shot_Object_Detection_via_Neural_Instance_Feature_Forging.pdf (4,912,178 bytes)	1							Yes	Yes
34	3/5/2023 11:25:12 AM -08:00	4/10/2023 2:55:10 PM -07:00	Leveraging triplet loss for unsupervised action segmentation.	In this paper, we propose a novel fully unsupervised framework that learns action representations suitable for the action segmentation task from the single input video itself, without requiring any training data. Our method is a deep metric learning approach rooted in a shallow network with a triplet loss operating on similarity distributions and a novel triplet selection strategy that effectively models temporal and semantic priors to discover actions in the new representational space. Under these circumstances, we successfully recover temporal boundaries in the learned action representations with higher quality compared with existing unsupervised approaches. The proposed method is evaluated on two widely used benchmark datasets for the action segmentation task and it achieves competitive performance by applying a generic clustering algorithm on the learned representations.	Elena Belén Bueno Benito	ebueno@iri.upc.edu	Elena Belén Bueno Benito (Institut de Robòtica i Informàtica Industrial)*; Biel Tura Vecino (Amazon Alexa AI, Cambridge); Mariella Dimiccoli (CSIC-UPC)	Bueno Benito, Elena Belén*; Tura Vecino, Biel; Dimiccoli, Mariella	ebueno@iri.upc.edu*; bieltura@amazon.co.uk; mdimiccoli@iri.upc.edu			0	4	4	100	0	Disabled (0)	Accept (Archive)	No	No	Yes	Yes	No	CVPR23_L3D-IVU.pdf (10,483,668 bytes)	1		0	Jingkang Yang (Nanyang Technological University); Pierre-Luc St-Charles (Mila); Xinyue Huo (University of Science and Technology of China); Zhengfeng Lai (University of California, Davis)	jingkang001@e.ntu.edu.sg; pierreluc.stcharles@mila.quebec; xinyueh@mail.ustc.edu.cn; lzhengfeng@ucdavis.edu					Yes	No
37	3/5/2023 4:14:13 PM -08:00	4/10/2023 6:35:06 AM -07:00	Improving Automatic Target Recognition in Low Data Regime using Semi-Supervised Learning and Generative Data Augmentation	We propose a new strategy to improve Automatic Target Recognition (ATR) from infrared (IR) images by leveraging semi-supervised learning and generative data augmentation.   Our approach is twofold: first, we use an automatic detector's outputs to augment the existing labeled and unlabeled data. Second, we introduce a confidence-guided data generative augmentation technique that focuses on learning from the most challenging regions of the feature space, to generate synthetic data which can be used as extra unlabeled data.    We evaluate the proposed approach on a public dataset with IR imagery of civilian and military vehicles. We show that yields substantial percentage improvements in ATR performance relative to both the baseline fully supervised model trained using the existing data only, and a semi-supervised model trained without generative data augmentation. For instance, for the most challenging data partition, our method achieves a relative increase of 29.51% over the baseline fully supervised model and a relative improvement of 2.59% over the semi-supervised model. These results demonstrate the effectiveness of our approach in low-data regimes, where labeled data is limited or expensive to obtain.	Fadoua Khmaissia	f0khma01@louisville.edu	Fadoua Khmaissia (University of Louisville)*; Hichem Frigui (University of Louisville)	Khmaissia, Fadoua*; Frigui, Hichem	f0khma01@louisville.edu*; h.frigui@louisville.edu			0	4	4	100	0	Disabled (0)	Accept (Archive)	No	No	Yes	Yes	No	ATR___L3D_IVU_paper submission.pdf (614,553 bytes)	1		0	Abhishek Sinha (Stanford University); Divyansh Jha (Woven Planet Holdings); Jie Hong (Australian National University); Qiangqiang Wu (City University of Hong Kong)	a7b23@stanford.edu; divyanshj.16@gmail.com; jie.hong@anu.edu.au; qiangqwu2-c@my.cityu.edu.hk					Yes	No
39	3/5/2023 6:41:12 PM -08:00	4/10/2023 9:14:50 PM -07:00	In Defense of Structural Symbolic Representation for Video Event-Relation Prediction	Understanding event relationships in videos requires a model to understand the underlying structures of events (i.e. the event type, the associated argument roles, and corresponding entities) and factual knowledge for reasoning. Structural symbolic representation (SSR) based methods directly take event types and associated argument roles/entities as inputs to perform reasoning. However, the state-of-the-art video event-relation prediction system shows the necessity of  using continuous feature vectors from input videos; existing methods based solely on SSR inputs fail completely, even when given oracle event types and argument roles. In this paper, we conduct an extensive empirical analysis to answer the following questions: 1) why SSR-based method failed; 2) how to understand the evaluation setting of video event relation prediction properly; 3) how to uncover the potential of SSR-based methods. We first identify suboptimal training settings as causing the failure of previous SSR-based video event prediction models. Then through qualitative and quantitative analysis, we show how evaluation that takes only video as inputs is currently unfeasible, as well as the reliance on oracle event information to obtain an accurate evaluation. Based on these findings, we propose to further contextualize the SSR-based model to an Event-Sequence Model and equip it with more factual knowledge through a simple yet effective way of reformulating external visual commonsense knowledge bases into an event-relation prediction pretraining dataset. The resultant new state-of-the-art model eventually establishes a 25% Macro-accuracy performance boost.	Andrew Y Lu	ayl2148@columbia.edu	Andrew Y Lu (Columbia University)*; Xudong Lin (Columbia University); Yulei Niu (Columbia University); Shih-Fu Chang (Columbia University)	Lu, Andrew Y*; Lin, Xudong; Niu, Yulei; Chang, Shih-Fu	ayl2148@columbia.edu*; xudong.lin@columbia.edu; yn.yuleiniu@gmail.com; sc250@columbia.edu			0	4	3	75	0	Disabled (0)	Accept (Archive)	No	No	Yes	Yes	No	Video_Event_Relation_Prediction.pdf (1,500,532 bytes)	1	Video_Event_Relation_Prediction_supplementary.pdf (3,030,764 bytes)	1	Abhishek Sinha (Stanford University); Divyansh Jha (Woven Planet Holdings); Hala Lamdouar (University of Oxford); Jie Hong (Australian National University)	a7b23@stanford.edu; divyanshj.16@gmail.com; lamdouar@robots.ox.ac.uk; jie.hong@anu.edu.au					Yes	No
40	3/5/2023 7:10:26 PM -08:00	4/7/2023 10:26:53 AM -07:00	Language Models are Causal Knowledge Extractors for Zero-shot Video Question Answering	Causal Video Question Answering (CVidQA) queries not only association or temporal relations but also causal relations in a video. Existing question synthesis methods pre-trained question generation (QG) systems on reading comprehension datasets with text descriptions as inputs. However, QG models only learn to ask association questions (e.g., ``what is someone doing...'') and result in inferior performance due to the poor transfer of association knowledge to CVidQA, which focuses on causal questions like ``why is someone doing ...''.  Observing this, we proposed to exploit causal knowledge to generate question-answer pairs, and proposed a novel framework, Causal Knowledge Extraction from Language Models (CaKE-LM), leveraging causal commonsense knowledge from language models to tackle CVidQA. To extract knowledge from LMs, CaKE-LM generates causal questions containing two events with one triggering another (e.g., ``score a goal'' triggers ``soccer player kicking ball'') by prompting LM with the action (soccer player kicking ball) to retrieve the intention (to score a goal). CaKE-LM significantly outperforms conventional methods by 4% to 6% of zero-shot CVidQA accuracy on NExT-QA and Causal-VidQA datasets. We also conduct comprehensive analyses and provide key findings for future research. 	Hung-Ting Su	htsu@cmlab.csie.ntu.edu.tw	Hung-Ting Su (National Taiwan University)*; Yulei Niu (Columbia University); Xudong Lin (Columbia University); Winston H. Hsu (National Taiwan University); Shih-Fu Chang (Columbia University)	Su, Hung-Ting*; Niu, Yulei; Lin, Xudong; Hsu, Winston H.; Chang, Shih-Fu	htsu@cmlab.csie.ntu.edu.tw*; yn.yuleiniu@gmail.com; xudong.lin@columbia.edu; whsu@ntu.edu.tw; shih.fu.chang@columbia.edu			0	4	4	100	0	Disabled (0)	Accept (Archive)	No	No	Yes	Yes	No	CaKE_CVPRW23 (2).pdf (479,909 bytes)	1		0	Hala Lamdouar (University of Oxford); Jiarui Xu (University of California San Diego); Stefan Wolf (Fraunhofer Institute of Optronics, System Technologies and Image Exploitation); Xu Luo (University of Electronic Science and Technology of China)	lamdouar@robots.ox.ac.uk; jix026@ucsd.edu; stefan.wolf@iosb.fraunhofer.de; Frank.Luox@outlook.com					Yes	No
46	3/6/2023 6:59:36 AM -08:00	4/10/2023 2:52:59 AM -07:00	FODVid: Flow-guided Object Discovery in Videos	Segmentation of objects in a video is challenging due to the nuances such as motion blurring, parallax, occlusions, changes in illumination, etc. Instead of addressing these nuances separately, we focus on building a generalizable solution that avoids overfitting to the individual intricacies. Such a solution would also help us save enormous resources involved in human annotation of video corpora. To solve Video Object Segmentation (VOS) in an unsupervised setting, we propose a new pipeline (FODVid) based on the idea of guiding segmentation outputs using flow-guided graph-cut and temporal consistency. Basically, we design a segmentation model incorporating intra-frame appearance and flow similarities, and inter-frame temporal continuation of the objects under consideration. We perform an extensive experimental analysis of our straightforward methodology on the standard DAVIS16 video benchmark. Though simple, our approach produces results comparable (within a range of 2 mIoU) to the existing top approaches in unsupervised VOS. The simplicity and effectiveness of our technique opens up new avenues for research in the video domain.	Silky Singh	silky1708@gmail.com	Silky Singh (Adobe Systems)*; Shripad V Deshmukh (Adobe, MDSR Labs); Mausoom Sarkar (Adobe); Rishabh Jain (Adobe); Mayur Hemani (Adobe Systems); Balaji Krishnamurthy ()	Singh, Silky*; Deshmukh, Shripad V; Sarkar, Mausoom; Jain, Rishabh; Hemani, Mayur; Krishnamurthy, Balaji	silky1708@gmail.com*; deshmukh.shripad.v@gmail.com; msarkar@adobe.com; f2015550p@alumni.bits-pilani.ac.in; mayur@adobe.com; kbalaji@adobe.com			0	4	2	50	0	Disabled (0)	Accept (No Archive)	No	No	Yes	Yes	No	FODVid.pdf (2,472,567 bytes)	1		0	Boyu Yang (University of Chinese Academy of Sciences); Massimiliano Mancini (University of Trento); Nacim Belkhir (Safran); Youngtaek Oh (KAIST)	yangboyu18@mails.ucas.ac.cn; massimiliano.mancini@unitn.it; nacim.belkhir@safrangroup.com; youngtaek.oh@kaist.ac.kr					Yes	No
47	3/6/2023 7:06:28 AM -08:00	4/10/2023 9:04:08 AM -07:00	NamedMask: Distilling Segmenters from Complementary Foundation Models	The goal of this work is to segment and name regions of images without access to pixel-level labels during training. To tackle this task, we  construct segmenters by distilling the complementary strengths of two foundation models. The first, CLIP, exhibits the ability to assign names to image content but lacks an accessible representation of object structure. The second, DINO, captures the spatial extent of objects but has no knowledge of object names. Our method, termed NamedMask, begins by using CLIP to construct category-specific archives of images. These images are pseudo-labelled with a category-agnostic salient object detector bootstrapped from DINO, then refined by category-specific segmenters using the CLIP archive labels. Thanks to the high quality of the refined masks, we show that a standard segmentation architecture trained on these archives with appropriate data augmentation achieves impressive semantic segmentation abilities for both single-object and multi-object images. As a result, our proposed NamedMask performs favourably against a range of prior work on five benchmarks including the VOC2012, COCO and large-scale ImageNet-S datasets.	Gyungin Shin	gyungin@robots.ox.ac.uk	Gyungin Shin (University of Oxford)*; Weidi Xie (University of Oxford); Samuel Albanie (University of Cambridge)	Shin, Gyungin*; Xie, Weidi; Albanie, Samuel	gyungin@robots.ox.ac.uk*; weidi.xie@eng.ox.ac.uk; sma71@cam.ac.uk			1	4	3	75	0	Disabled (0)	Accept (Archive)	No	No	Yes	Yes	No	id_47_main.pdf (9,438,849 bytes)	1	id_47_supplementary.pdf (8,472,058 bytes)	1	Hmrishav Bandyopadhyay (Jadavpur University); Massimiliano Mancini (University of Trento); Nacim Belkhir (Safran); Youngtaek Oh (KAIST)	hmrishavbandyopadhyay@gmail.com; massimiliano.mancini@unitn.it; nacim.belkhir@safrangroup.com; youngtaek.oh@kaist.ac.kr					Yes	No
48	3/6/2023 8:40:01 AM -08:00	4/8/2023 7:45:17 AM -07:00	LSFSL: Leveraging Shape Information in Few-shot Learning	Few-shot learning (FSL) techniques seek to learn the underlying patterns in data using fewer samples, analogous to how humans learn from limited experience. In this limited-data scenario, the challenges associated with deep neural networks, such as shortcut learning and texture bias behaviors, are further exacerbated. Moreover, the significance of addressing shortcut learning is not yet fully explored in the few-shot setup. To address these issues, we propose LSFSL, which enforces the model to learn more generalizable features utilizing the implicit prior information present in the data. Through comprehensive analyses, we demonstrate that LSFSL-trained models are less vulnerable to alteration in color schemes, statistical correlations, and adversarial perturbations leveraging the global semantics in the data. Our findings highlight the potential of incorporating relevant priors in few-shot approaches to increase robustness and generalization.	Deepan Chakravarthi Padmanabhan	deepangrad@gmail.com	Deepan Chakravarthi Padmanabhan (NavInfo Europe B.V.)*; Shruthi Gowda ( Navinfo Europe); Elahe Arani (Navinfo Europe ); Bahram Zonooz (Navinfo Europe)	Padmanabhan, Deepan Chakravarthi*; Gowda, Shruthi; Arani, Elahe; Zonooz, Bahram	deepangrad@gmail.com*; shruthi.gowda@navinfo.eu; e.arani@gmail.com; bahram.zonooz@gmail.com			0	4	3	75	0	Disabled (0)	Accept (Archive)	No	No	Yes	Yes	No	2023_-_Leveraging_Shape_Information_in_Few-shot_Learning.pdf (731,346 bytes)	1	2023_-_Leveraging_Shape_Information_in_Few-shot_Learning_Supplementary_Material.pdf (1,817,152 bytes)	1	Carlos Lassance (Naver Labs); Daniel Rebain (Google Inc.); Hmrishav Bandyopadhyay (Jadavpur University); Saurav Gupta (Mercedes Benz Research and Development India)	carlos.lassance@naverlabs.com; drebain@cs.ubc.ca; hmrishavbandyopadhyay@gmail.com; sauravgupta.iitd@gmail.com					Yes	No
49	3/6/2023 12:30:32 PM -08:00	4/8/2023 11:00:04 AM -07:00	Reliable Student: Addressing Noise in Semi-Supervised 3D Object Detection	Semi-supervised 3D object detection can benefit from the promising pseudo-labeling technique when labeled data is limited. However, recent approaches have overlooked the impact of noisy pseudo-labels during training, despite efforts to enhance pseudo-label quality through confidence-based filtering. In this paper, we examine the impact of noisy pseudo-labels on IoU-based target assignment and propose the Reliable Student framework, which incorporates two complementary approaches to mitigate errors. First, it involves a class-aware target assignment strategy that reduces false negative assignments in difficult classes. Second, it includes a reliability weighting strategy that suppresses false positive assignment errors while also addressing remaining false negatives from the first step. The reliability weights are determined by querying the teacher network for confidence scores of the student-generated proposals. Our work surpasses the previous state-of-the-art on KITTI 3D object detection benchmark on point clouds in the semi-supervised setting. On 1% labeled data, our approach achieves a 6.2% AP improvement for the pedestrian class, despite having only 37 labeled samples available. The improvements become significant for the 2% setting, achieving 6.0% AP and 5.7% AP improvements for the pedestrian and cyclist classes, respectively. Our code will be released at https://github.com/fnozarian/ReliableStudent	Farzad Nozarian	farzad.nozarian@dfki.de	Farzad Nozarian (DFKI)*; Shashank Agarwal (DFKI); Farzaneh Rezaeianaran (DFKI); Danish Shahzad (DFKI); Atanas Poibrenski (Deutsches Forschungszentrum für Künstliche Intelligenz (DFKI), Saarland Informatics Campus ); Christian Mueller (DFKI); Philipp Slusallek (German Research Center for Artificial Intelligence (DFKI) & Saarland University)	Nozarian, Farzad*; Agarwal, Shashank; Rezaeianaran, Farzaneh; Shahzad, Danish; Poibrenski, Atanas; Mueller, Christian; Slusallek, Philipp	farzad.nozarian@dfki.de*; shashank.agarwal@dfki.de; farzaneh.rezaeianaran@dfki.de; danish.shahzad@dfki.de; atanas.poibrenski@dfki.de; christian.mueller@dfki.de; philipp.slusallek@dfki.de			0	4	3	75	0	Disabled (0)	Accept (Archive)	No	No	Yes	Yes	No	Reliable Student_Addressing Noise in Semi-Supervised 3D Object Detection.pdf (848,218 bytes)	1		0	Carlos Lassance (Naver Labs); Daniel Rebain (Google Inc.); Karim Guirguis (Robert Bosch Coorporate Research); Saurav Gupta (Mercedes Benz Research and Development India)	carlos.lassance@naverlabs.com; drebain@cs.ubc.ca; karim.guirguis@de.bosch.com; sauravgupta.iitd@gmail.com					Yes	No
50	3/6/2023 5:55:15 PM -08:00	4/10/2023 1:44:40 AM -07:00	Zero-shot Object Classification with Large-scale Knowledge Graph	Zero-shot learning is used to predict unseen categories and can solve problems such as dealing with unseen categories that were not anticipated at the time of training and the lack of labeled datasets. One method for zero-shot object classification is to use a knowledge graph, which is a set of explicit knowledge. Because recognition is limited to the categories contained in the knowledge graph, and the relationships among categories are expected to be quantitatively and qualitatively richer depending on the graph size, it is desirable to handle a large-scale knowledge graph that contains as many categories as possible.We used a knowledge graph that contains approximately seven times as many categories as the knowledge graphs used mainly in existing research to enable the classification of a larger number of categories and to achieve more accurate recognition.When using a large-scale knowledge graph, the number of noisy nodes and edges is expected to increase.Therefore, we propose a method to extract useful information from the entire graph using positional relationships between categories and types of edges in the knowledge graph. We classify images that were earlier unclassifiable in existing research and show that the proposed data extraction method improves performance compared to using the entire graph.	Kohei Shiba	shiba@mi.t.u-tokyo.ac.jp	Kohei Shiba (University of Tokyo)*; Yusuke Mukuta (The University of Tokyo); Tatsuya Harada (The University of Tokyo / RIKEN)	Shiba, Kohei*; Mukuta, Yusuke; Harada, Tatsuya	shiba@mi.t.u-tokyo.ac.jp*; mukuta@mi.t.u-tokyo.ac.jp; harada@mi.t.u-tokyo.ac.jp			0	4	3	75	0	Disabled (0)	Accept (Archive)	No	No	Yes	Yes	No	Zero_shot_Object_Classification_with_Large_scale_Knowledge_Graph.pdf (3,508,478 bytes)	1		0	Jiajun Shen (TCL Research); Karim Guirguis (Robert Bosch Coorporate Research); MANISIMHA VARMA MANTHENA (Manipal institute of technology); Zhexiong Liu (University of Pittsburgh)	shenjiajun90@gmail.com; karim.guirguis@de.bosch.com; manthenasimha@gmail.com; zhexiong@cs.pitt.edu					Yes	No
53	3/6/2023 8:40:43 PM -08:00	4/10/2023 2:52:54 AM -07:00	Knowledge Assembly: Semi-Supervised Multi-Task Learning from Multiple Datasets with Disjoint Labels	In real-world scenarios we often need to perform multiple tasks simultaneously. Multi-Task Learning (MTL) is an adequate method to do so, but usually requires datasets labeled for all tasks. We propose a method that can leverage datasets labeled for only some of the tasks in the MTL framework. Our work, Knowledge Assembly (KA), learns multiple tasks from disjoint datasets by leveraging the unlabeled data in a semi-supervised manner, using model augmentation for pseudo-supervision. Whilst KA can be implemented on any existing MTL networks, we test our method on person re-identification (reID) and pedestrian attribute recognition (PAR). We surpass the single task fully-supervised performance by 4.2% points for reID and 0.9% points for PAR.	Federica  Spinola	federica.spinola@deepingsource.io	Federica  Spinola (Deeping Source )*; Philipp Benz (KAIST); Minhyeong Yu (DeepingSource Inc.); Tae-hoon Kim (Deeping Source Inc.)	Spinola, Federica *; Benz, Philipp; Yu, Minhyeong; Kim, Tae-hoon	federica.spinola@deepingsource.io*; phibenz@gmail.com; minhyeong.yu@deepingsource.io; pete.kim@deepingsource.io			1	4	3	75	0	Disabled (0)	Accept (No Archive)	No	No	Yes	No	No	53_Knowledge_Assembly_CVPRW.pdf (2,834,272 bytes)	1		0	Chao Wang (Southern University of Science and Technology); Jike Zhong (The Ohio State University); Jiyang  Zheng (University of Sydney); Viveka Kulharia (University of Oxford)	chaowang.hk@gmail.com; zhong.523@osu.edu; jzhe5740@uni.sydney.edu.au; vivek.akulharia@gmail.com					No	No
57	3/6/2023 9:45:19 PM -08:00	5/3/2023 3:34:08 PM -07:00	Stream-Based Active Distillation for Scalable Model Deployment	This paper proposes a scalable technique for developing lightweight yet powerful models for object detection in videos using self-training with knowledge distillation. This approach involves training a compact student model using pseudo-labels generated by a computationally complex but generic teacher model, which can help to reduce the need for massive amounts of data and computational power. However, model-based annotations in large-scale applications may propagate errors or biases. To address these issues, our paper introduces Stream-Based Active Distillation (SBAD) to endow pre-trained students with effective and efficient fine-tuning methods that are robust to teacher imperfections. The proposed pipeline: (i) adapts a pre-trained student model to a specific use case, based on a set of frames whose pseudo-labels are predicted by the teacher, and (ii) selects on-the-fly, along a streamed video, the images that should be considered to fine-tune the student model. Various selection strategies are compared, demonstrating: 1) the effectiveness of implementing distillation with pseudo-labels, and 2) the importance of selecting images for which the pre-trained student detects with a high confidence.	Dani Manjah	dani.manjah@uclouvain.be	Dani Manjah (Université Catholique de Louvain )*; Davide Cacciarelli (Technical University of Denmark); Baptiste Standaert (Université Catholique de Louvain); Mohamed Benkedadra (UMONS); Gauthier Rotsart de Hertaing (UCLouvain); Benoit Macq (Université Catholique de Louvain); Stéphane Galland (UTBM); Christophe De Vleeschouwer (Université Catholique de Louvain)	Manjah, Dani*; Cacciarelli, Davide; Standaert, Baptiste; Benkedadra, Mohamed; Rotsart de Hertaing, Gauthier; Macq, Benoit; Galland, Stéphane; De Vleeschouwer, Christophe	dani.manjah@uclouvain.be*; dcac@dtu.dk; baptiste.standaert@uclouvain.be; mohamed.benkedadra@umons.ac.be; gauthier.rotsart@uclouvain.be; benoit.macq@uclouvain.be; stephane.galland@utbm.fr; Christophe.Devleeschouwer@uclouvain.be			0	4	4	100	0	Disabled (0)	Accept (Archive)	No	No	Yes	Yes	No	Stream-Based Active Distillation for scalable model deployment.pdf (4,298,215 bytes)	1		0	Clement Pinard (XXII); Dryden Wiebe (UBC); Shweta Mahajan (University of British Columbia); Zhijie Wu (University of British Columbia)	clempinard@gmail.com; dryden.wiebe@gmail.com; shweta.nith@outlook.com; zhijiewu@cs.ubc.ca					Yes	No
59	3/6/2023 11:34:43 PM -08:00	4/8/2023 5:25:03 PM -07:00	Incorporating Visual Grounding In GCN For Zero-shot Learning Of Human Object Interaction Actions	GCN-based zero-shot learning approaches commonly use fixed input graphs representing external knowledge that usually comes from language. However, such input graphs fail to incorporate the visual domain nuances. We introduce a method to ground the external knowledge graph visually. The method is demonstrated on a novel concept of grouping actions according to a shared notion and shown to be of superior performance in zero-shot action recognition on two challenging human manipulation action datasets, the EPIC Kitchens dataset, and the Charades dataset. We further show that visually grounding the knowledge graph enhances the performance of GCNs when an adversarial attack corrupts the input graph.	Chinmaya Devaraj	chinmayd@umd.edu	Chinmaya Devaraj (Univ of Maryland)*; Cornelia Fermuller (University of Maryland, College Park); Yiannis Aloimonos (University of Maryland, College Park)	Devaraj, Chinmaya*; Fermuller, Cornelia; Aloimonos, Yiannis	chinmayd@umd.edu*; fer@cfar.umd.edu; jyaloimo@umd.edu			0	7	2	28	0	Disabled (0)	Accept (Archive)	No	No	Yes	Yes	No	CVPR23w_GCN.pdf (790,300 bytes)	1	Submission_59.zip (832,197 bytes)	1	David Vazquez (Element AI); Federico Danieli (Apple); Gabriel Huang (Mila & University of Montreal); Juana Valeria Hurtado (UNIVERSTY OF FREIBURG); Mennatullah Siam (York University); Touqeer Ahmad (University of Colorado, Colorado Springs); Xilin Chen (Institute of Computing Technology, Chinese Academy of Sciences)	aklaway@gmail.com; federico.danieli.90@gmail.com; gbxhuang@gmail.com; hurtadoj@cs.uni-freiburg.de; mennatul@ualberta.ca; sh.touqeerahmad@gmail.com; xlchen@ict.ac.cn					Yes	No
